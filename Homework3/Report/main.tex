\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 

% Unfortunately, LaTeX has a hard time interpreting German Umlaute. The following two lines and packages should help. If it doesn't work for you please let me know.
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}

% The following two packages - multirow and booktabs - are needed to create nice looking tables.
\usepackage{multirow} % Multirow is for tables with multiple rows within one cell.
\usepackage{booktabs} % For even nicer tables.
\usepackage{amsmath}
\usepackage{bm}

% As we usually want to include some plots (.pdf files) we need a package for that.
\usepackage{graphicx} 
\usepackage{rotating}
\usepackage{cancel}

% The default setting of LaTeX is to indent new paragraphs. This is useful for articles. But not really nice for homework problem sets. The following command sets the indent to 0.
\usepackage{setspace}
\setlength{\parindent}{0in}

% Package to place figures where you want them.
\usepackage{float}

% The fancyhdr package let's us create nice headers.
\usepackage{fancyhdr}

\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{makecell}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\renewcommand{\arraystretch}{1.5}

\pagestyle{fancy} % With this command we can customize the header style.

\fancyhf{} % This makes sure we do not have other information in our header or footer.

\lhead{\footnotesize Homework 3}% \lhead puts text in the top left corner. \footnotesize sets our font to a smaller size.

%\rhead works just like \lhead (you can also use \chead)
\rhead{\footnotesize Joana Pimenta, Rodrigo Laia} %<---- Fill in your lastnames.

% Similar commands work for the footer (\lfoot, \cfoot and \rfoot).
% We want to put our page number in the center.
\cfoot{\footnotesize \thepage} 

\begin{document}

\thispagestyle{empty} % This command disables the header on the first page. 

\begin{tabular}{p{15.5cm}} % This is a simple tabular environment to align your text nicely 
{\large \bf Aprendizagem} \\
Instituto Superior Técnico \\ outubro  de 2023  \\ \\ 
\hline % \hline produces horizontal lines.
\\
\end{tabular} % Our tabular environment ends here.

\vspace*{0.3cm} % Now we want to add some vertical space in between the line and our title.

\begin{center} % Everything within the center environment is centered.
	{\Large \bf Homework 3 - Report} % <---- Don't forget to put in the right number
	\vspace{2mm}
	
        % YOUR NAMES GO HERE
	{\bf Joana Pimenta (103730), Rodrigo Laia (102674) } % <---- Fill in your names here!
		
\end{center}  

\vspace{0.4cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Up until this point you only have to make minor changes for every week (Number of the homework). Your write up essentially starts here.

\section*{Pen and Paper}
\begin{enumerate}

\item 

% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
%  & $y_1$ & $y_2$ & $y_{out}$ \\ \hline
% $x_1$ & 0.7 & -0.3 & 0.8 \\ \hline
% $x_2$ & 0.4 & 0.5 & 0.6 \\ \hline
% $x_3$ & -0.2 & 0.8 & 0.3 \\ \hline
% $x_4$ & -0.4 & 0.3 & 0.3 \\ \hline
% \end{tabular}
% \end{table}


%%%%%%%%%%%%%%%%%% a) %%%%%%%%%%%%%%%%%%%

a) Uma função radial basis permite mapear observações para um novo espaço 
baseando-se na distância entre as observações e os centróides.

\begin{equation}
    \phi_j (x) = \exp\left(-\frac{\left\|\vec{x}-c_j\right\|^2}{2}\right)
\end{equation}

O cálculos dos vetores tranformados foi feita através da seguinte fórmula:

\begin{equation}
\vec{\phi_i} = \left( \exp\left(-\frac{\left\|\vec{x}_i-c_1\right\|^2}{2}\right)   , \exp\left(-\frac{\left\|\vec{x}_i-c_2\right\|^2}{2}\right)  , \exp\left(-\frac{\left\|\vec{x}_i-c_3\right\|^2}{2}\right)   \right)
\end{equation}

Assim os vetores transformados obtidos foram:
\begin{equation*}
    \phi_1 = (0.74826,0.74826,0.10127)
\end{equation*}

\begin{equation*}
    \phi_2 = (0.81465,0.27117,0.33121)
\end{equation*}

\begin{equation*}
    \phi_3 = (0.71177,0.09633,0.71177)
\end{equation*}

Para fazer regressão de Ridge é necessário minimizar a função de erro:

\begin{equation}
    E(\vec{w}) = \frac{1}{2} \sum_{i=1}^{n} (z_i - \vec{w}^T \cdot x_i)^2 + \frac{\lambda}{2} ||\vec{w}||^2
\end{equation}

Sendo que isso é equivalente a calcular $\vec{w}$ através da seguinte fórmula:

\begin{equation}
    \vec{w} = (X^T\cdot X + \lambda \cdot I)^{-1} \cdot X^T \cdot \vec{z}
\end{equation}

Uma vez que estamos a trabalhar com uma transformação de espaços, é necessário
calcular a matriz transformada $\Phi$ colocando para cada linha um 1 na primeira
coluna e depois o vetor transformado de cada observação. Utilizamos então as 
fórmulas acima com $\Phi$ no lugar de $X$, assumindo que após a transformação a 
relação entre as variáveis e o target é linear.\\ 

Cálculos intermédios:

\begin{equation*}
    \Phi = 
\begin{bmatrix}
    1 & 0.74826 & 0.74826 & 0.10127 \\ 1 & 0.81465 & 0.27117 & 0.33121 \\ 1 & 0.71177 & 0.09633 & 0.71177 \\ 1 & 0.88250 & 0.16122 & 0.65377
\end{bmatrix}
\end{equation*}

\begin{equation*}
    \Phi^T = \begin{bmatrix}
    1 & 1 & 1 & 1 \\ 0.74826 & 0.81465 & 0.71177 & 0.88250 \\ 0.74826 & 0.27117 & 0.09633 & 0.16122 \\ 0.10127 & 0.33121 & 0.71177 & 0.65377
    \end{bmatrix}
\end{equation*}

\begin{equation*}
    (\Phi^T\cdot \Phi + \lambda \cdot I)^{-1} \cdot \Phi^T = \begin{bmatrix}  0.14105 & 0.35022 & 0.35575 & -0.30185 \\
        -0.09064 & 0.43823 & -0.50361 & 0.53370 \\
         0.99394 & -0.50615  & -0.13690 & -0.16477 \\
        -0.31222 & -0.65246 & 0.72647  &  0.42436 \\ \end{bmatrix}
\end{equation*}

\begin{equation*}
    \vec{w} = \begin{bmatrix}  0.33914 \\ 0.19945 \\ 0.40096 \\ -0.29600 \end{bmatrix}
\end{equation*}

Assim, a regressão de Ridge obtida foi:

\begin{equation*}  
    \hat{z} = 0.33914 + 0.19945 \cdot \phi_1 + 0.40096 \cdot \phi_2 - 0.29600 \cdot \phi_3
\end{equation*}

%%%%%%%%%%%%%%%%%% b) %%%%%%%%%%%%%%%%%%%
b)
Para calcular o RMSE (root mean square error) foi utilizada a seguinte fórmula:

\begin{equation}
    RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (z_i - \hat{z}_i)^2}
\end{equation}
em que,
\begin{equation}
    \hat{z}_i = \vec{w}^T \cdot \vec{\phi_i}
\end{equation}
 
Targets estimados:
\begin{equation*}
    \hat{z}_1 = 0.75844
\end{equation*}

\begin{equation*}
    \hat{z}_2 = 0.51232
\end{equation*}

\begin{equation*}
    \hat{z}_3 = 0.30905
\end{equation*}

\begin{equation*}
    \hat{z}_4 = 0.38629
\end{equation*}

Assim, o RMSE obtido foi:
\begin{equation*}
    RMSE = 0.06508
\end{equation*}

\item
É importante referir que para este exercício se utilizou a seguinte notação: $L^{\text{label}}_{observation}$

As fórmulas utilizadas foram:

\begin{equation}
    \textbf{x}^{[p]} = \phi({W}^{[p]} \cdot \textbf{x}^{[p-1]} + \textbf{b}^{[p]})
\end{equation}

\begin{equation}
    \bm{\delta}^{[p]} = \frac{\partial E}{\partial \textbf{x}^{[p]}} \circ \frac{\partial \textbf{x}^{[p]}}{\partial \textbf{z}^{[p]}} = (\textbf{x}^{[p]} - \mathbf{t}) \circ \phi'(\mathbf{z}^{[p]}) \text{, para a última layer}
\end{equation}    

\begin{equation}
    \bm{\delta}^{[p]} = (\frac{\partial \textbf{z}^{[p+1]}}{\partial \textbf{x}^{[p]}})^T \cdot \bm{\delta}^{[p+1]} \circ \frac{\partial \textbf{x}^{[p]}}{\partial \textbf{z}^{[p]}} = ({W}^{[p+1]})^T \cdot \bm{\delta}^{[p+1]} \circ \phi'(\textbf{z}^{[p]}) \text{, para as outras layers}
\end{equation}

\begin{equation}
    {W}^{[p]} = {W}^{[p]} - \eta \cdot \frac{\partial E}{\partial {W}^{[p]}} = {W}^{[p]} - \eta \cdot \bm{\delta}^{[p]} \cdot (\textbf{x}^{[p-1]})^T
\end{equation}

\begin{equation}
    \textbf{b}^{[p]} = \textbf{b}^{[p]} - \eta \cdot \frac{\partial E}{\partial \textbf{b}^{[p]}} = \textbf{b}^{[p]} - \eta \cdot \bm{\delta}^{[p]}
\end{equation}

Estas expressões são válidas para squared error loss function.

Dados necessários para começar o algoritmo:

\begin{equation*}
    \textbf{x}^{[0]}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} , \textbf{t}_1 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} , \textbf{x}^{[0]}_2 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ -1 \end{bmatrix} , \textbf{t}_2 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
\end{equation*}

\begin{equation*}
    W^{[1]} = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 1 \\ 1 & 1 & 1 & 1 \end{bmatrix} , \textbf{b}^{[1]} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
\end{equation*}

\begin{equation*}
    W^{[2]} = \begin{bmatrix} 1 & 4 & 1 \\  1 & 1 & 1 \end{bmatrix} , \textbf{b}^{[2]} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
\end{equation*}

\begin{equation*}
    W^{[3]} = \begin{bmatrix} 1 & 1 \\ 3 & 1 \\ 1 & 1 \end{bmatrix} , \textbf{b}^{[3]} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \phi = tanh(0.5x -2) \text{, } \phi' = 0.5 * (1 - tanh^2(0.5x -2))
\end{equation*}

Guia de rede considerado:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{rede.jpg}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{equacoes.jpg}
\end{figure}

Primeiro é necessário realizar (forward) propagation para obter os valores das observações:

- Para a primeira observação:

\begin{equation*}
    \textbf{z}^{[1]}_1 = W^{[1]} \cdot \textbf{x}^{[0]}_1 + \textbf{b}^{[1]} = \begin{bmatrix} 5 \\ 6 \\ 5 \end{bmatrix} \implies \textbf{x}^{[1]}_1 = \phi(\textbf{z}^{[1]}_1)= \begin{bmatrix} 0.46212 \\ 0.76159 \\ 0.46212 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{z}^{[2]}_1 = W^{[2]} \cdot \textbf{x}^{[1]}_1 + \textbf{b}^{[2]} = \begin{bmatrix} 4.97061 \\ 2.68583 \end{bmatrix} \implies \textbf{x}^{[2]}_1 = \phi(\textbf{z}^{[2]}_1)= \begin{bmatrix} 0.45048 \\ -0.57642 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{z}^{[3]}_1 = W^{[3]} \cdot \textbf{x}^{[2]}_1 + \textbf{b}^{[3]} = \begin{bmatrix} 0.87406 \\ 1.77503 \\ 0.87406 \end{bmatrix} \implies \textbf{x}^{[3]}_1 = \phi(\textbf{z}^{[3]}_1)= \begin{bmatrix} -0.91590 \\ -0.80494\\ -0.91590 \end{bmatrix}
\end{equation*}

- Para a segunda observação:

\begin{equation*}
    \textbf{z}^{[1]}_2 = W^{[1]} \cdot \textbf{x}^{[0]}_2 + \textbf{b}^{[1]} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \implies \textbf{x}^{[1]}_2 = \phi(\textbf{z}^{[1]}_2)= \begin{bmatrix} -0.90515 \\ -0.90515 \\ -0.90515 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{z}^{[2]}_2 = W^{[2]} \cdot \textbf{x}^{[1]}_2 + \textbf{b}^{[2]} = \begin{bmatrix} -4.43089 \\ -1.71545 \end{bmatrix} \implies \textbf{x}^{[2]}_2 = \phi(\textbf{z}^{[2]}_2)= \begin{bmatrix} -0.99956 \\ -0.99343 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{z}^{[3]}_2 = W^{[3]} \cdot \textbf{x}^{[2]}_2 + \textbf{b}^{[3]} = \begin{bmatrix} -0.99300 \\ -2.99212 \\ -0.99300 \end{bmatrix} \implies \textbf{x}^{[3]}_2 = \phi(\textbf{z}^{[3]}_2)= \begin{bmatrix} -0.98652 \\ -0.99816\\ -0.98652 \end{bmatrix}
\end{equation*}

Depois é necessário realizar (backward) propagation dos erros da última camada para a primeira:

- Para a primeira observação:

\begin{equation*}
    \bm{\delta}^{[3]}_1 = (\textbf{x}^{[3]}_1 - \textbf{t}_1) \circ \phi'(\textbf{z}^{[3]}_1) = \begin{bmatrix} -0.07379 \\ -0.31773 \\ -0.07379 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \bm{\delta}^{[2]}_1 = (W^{[3]})^T \cdot \bm{\delta}^{[3]}_1 \circ \phi'(\textbf{z}^{[2]}_1) = \begin{bmatrix} -0.43870\\ -0.15535 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \bm{\delta}^{[1]}_1 = (W^{[2]})^T \cdot \bm{\delta}^{[2]}_1 \circ \phi'(\textbf{z}^{[1]}_1) = \begin{bmatrix} -0.23359 \\ -0.40110 \\ -0.23359 \end{bmatrix}
\end{equation*}

- Para a segunda observação:

\begin{equation*}
    \bm{\delta}^{[3]}_2 = (\textbf{x}^{[3]}_2 - \textbf{t}_2) \circ \phi'(\textbf{z}^{[3]}_2) = \begin{bmatrix} -0.02660 \\ -0.00183 \\ -0.01321 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \bm{\delta}^{[2]}_2 = (W^{[3]})^T \cdot \bm{\delta}^{[3]}_2 \circ \phi'(\textbf{z}^{[2]}_2) = \begin{bmatrix} -1.97439\cdot 10^{-5} \\ -2.72552\cdot 10^{-4} \end{bmatrix}
\end{equation*}

\begin{equation*}
    \bm{\delta}^{[1]}_2 = (W^{[2]})^T \cdot \bm{\delta}^{[2]}_2 \circ \phi'(\textbf{z}^{[1]}_2) = \begin{bmatrix} -2.64099\cdot 10^{-5} \\ -3.17617\cdot 10^{-5} \\ -2.64099\cdot 10^{-5} \end{bmatrix}
\end{equation*}

Por fim é necessário atualizar os pesos e os bias. Como estamos a realizar um batch gradient descent update (com learning rate igual a 0.1), a expressão para os pesos atualizados é:

\begin{equation}
    W^{[p]} = W^{[p]} - \eta \cdot (\bm{\delta}^{[p]}_1 \cdot (\textbf{x}^{[p-1]}_1)^T + \bm{\delta}^{[p]}_2 \cdot (\textbf{x}^{[p-1]}_2)^T)
\end{equation}

Assim, os pesos e os bias atualizados foram:

\begin{equation*}
    W^{[1]} = W^{[1]} - 0.1 \cdot (\bm{\delta}^{[1]}_1 \cdot (\textbf{x}^{[0]}_1)^T + \bm{\delta}^{[1]}_2 \cdot (\textbf{x}^{[0]}_2)^T) = \begin{bmatrix} 1.02336 & 1.02336 & 1.02336 & 1.02336 \\
                                                                                                                            1.04011 & 1.04011 & 2.04011 & 1.04011 \\
                                                                                                                            1.02336 & 1.02336 & 1.02336 & 1.02336 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{b}^{[1]} = \textbf{b}^{[1]} - 0.1 \cdot (\bm{\delta}^{[1]}_1 + \bm{\delta}^{[1]}_2) = \begin{bmatrix} 1.02336 \\ 1.04011 \\ 1.02336 \end{bmatrix}
\end{equation*}

\begin{equation*}
    W^{[2]} = W^{[2]} - 0.1 \cdot (\bm{\delta}^{[2]}_1 \cdot (\textbf{x}^{[1]}_1)^T + \bm{\delta}^{[2]}_2 \cdot (\textbf{x}^{[1]}_2)^T) = \begin{bmatrix} 1.02027 & 4.03341 & 1.02027 \\
                                                                                                                            1.00715 & 1.01181 & 1.00715 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{b}^{[2]} = \textbf{b}^{[2]} - 0.1 \cdot (\bm{\delta}^{[2]}_1 + \bm{\delta}^{[2]}_2) = \begin{bmatrix} 1.04387 \\ 1.01556 \end{bmatrix}
\end{equation*}

\begin{equation*}
    W^{[3]} = W^{[3]} - 0.1 \cdot (\bm{\delta}^{[3]}_1 \cdot (\textbf{x}^{[2]}_1)^T + \bm{\delta}^{[3]}_2 \cdot (\textbf{x}^{[2]}_2)^T) = \begin{bmatrix} 1.00067 & 0.99310 \\
                                                                                                                            3.01413 & 0.98150 \\
                                                                                                                            1.00200 & 0.99443 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \textbf{b}^{[3]} = \textbf{b}^{[3]} - 0.1 \cdot (\bm{\delta}^{[3]}_1 + \bm{\delta}^{[3]}_2) = \begin{bmatrix} 1.01004 \\ 1.03196 \\ 1.00870 \end{bmatrix}
\end{equation*}

\end{enumerate}

\clearpage
\section*{Programming - Código Python e Resultados Obtidos}

\begin{enumerate}

\item Utilizamos um training-test split de 80-20, ou seja 80\% dos dados foram utilizados para treinar o modelo e 20\% para testá-lo.\\ 
De modo a minimizar o erro devido à aleatoriedade do algoritmo, o output final considerado foi a média de 10 outputs de 10 regressores MLP estocásticos com sementes de 1 a 10.
Os resíduos foram calculados através da seguinte fórmula:

\begin{equation}
    \text{residual} = | z - \hat{z} |
\end{equation}

O histograma dos resíduos encontra-se representado na seguinte figura:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ex1_histogram.png}
\end{figure}

Observando o histograma, conclui-se que a frequência diminui significativamente 
com o crescimento do valor absoluto dos resíduos. Isto significa que a maior parte
da diferença entre o valor real e o valor previsto pelo MLP é pequena.\\ \\

Códio Utilizado:

\begin{lstlisting}[language=Python]
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor

# Reading the csv file
df = pd.read_csv('Homework3/winequality-red.csv')
# Separating the variables from the target
variables = df.drop("quality", axis= 1)
target = df['quality']

# Training Test Split
variables_train, variables_test, target_train, target_test= train_test_split(variables, target, 
                                                                            train_size=0.8, stratify=target, random_state=0)

y_pred = np.zeros(len(target_test))

# Average the mlp regressor
for i in range(1, 11):
    # Learn the MLP regressor 
    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', early_stopping=True, validation_fraction=0.2, random_state=i)
    #Predict output
    y_pred += mlp.fit(variables_train,target_train).predict(variables_test)

y_pred = y_pred/10
first_rmse = np.sqrt(np.mean((target_test - y_pred)**2))

######### Exercise 1 ##########

# Calculate the residues
residues = abs(target_test - y_pred)
# Plot the residues

plt.hist(residues, edgecolor='darkblue', bins=20)
plt.title('Histogram of the residues')
plt.xlabel('Residues')
plt.ylabel('Frequency')
plt.savefig('ex1_histogram.png')
plt.show()
    
\end{lstlisting}

\item Uma vez que sabemos que a qualidade do vinho tem de ser um inteiro entre 
1 e 10 podemos arredondar os valores previstos pelo MLP para o inteiro mais próximo
e transformar os valores menores que 1 em 1 e os maiores que 10 em 10.\\ Comparando 
os valores do erro médio absoluto (MAE) antes e depois deste processo conclui-se 
que o MAE antes (0.51167) é maior do que o novo (0.45625). Assim, podemos dizer que 
este processo melhora o desempenho do MLP.\\

Código Utilizado:

\begin{lstlisting}[language=Python]

########## Exercise 2 ##########

# Round and bound the predictions
rounded_predictions = np.round(y_pred)
min_value = 1
max_value = 10 
rounded_and_bounded_predictions = np.clip(rounded_predictions, min_value, max_value)

# Calculate previous MAE
mae = np.mean(abs(target_test - y_pred))
#Calculate new MAE
mae_new = np.mean(abs(target_test - rounded_and_bounded_predictions))

print('The previous MAE is: ', mae)
print('The new MAE is: ', mae_new)

if mae_new < mae:
    print('The new MAE is lower than the previous one')
elif mae_new > mae:
    print('The new MAE is higher than the previous one')
else:
    print('The new MAE is equal to the previous one')
\end{lstlisting}

\item Até agora foram considerados MLP's com early stopping. Neste exercício vamos
ver qual é o efeito do número máximo de iterações no desempenho do MLP, avaliado através do RMSE.
Assim foram considerados 4 MLP's com números máximos de iterações iguais a 20, 50, 100 e 200. 
Da mesma maneira que antes, o modelo final considerado foi a média de 10 modelos com sementes de 1 a 10. 
No gráfico seguinte encontra-se representado o RMSE em função do número máximo de iterações.\\ \\

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{ex3_rmse.png}
\end{figure}

Observando o gráfico, conclui-se que o RMSE diminui significativamente com o aumento do número máximo de iterações.
No entanto, essa diminuição abranda à medida que se aumenta o número máximo de iterações.\\

Código utilizado:

\begin{lstlisting}[language=Python]
# Calculate the RMSE for each number of iterations
rmse_final = []
iter_array = [20,50,100,200]
for iter in iter_array:
    y_pred = np.zeros(len(target_test))
    for i in range(1, 11):
        # Learn the MLP regressor 
        mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', solver='adam', max_iter = iter, random_state=i)
        #Predict output
        y_pred += mlp.fit(variables_train,target_train).predict(variables_test)
    y_pred = y_pred/10
    rmse_final.append(np.sqrt(np.mean((y_pred-target_test)**2)))

def const(x):
    return first_rmse

# Plot the RMSE
plt.plot(iter_array, rmse_final, '-o', label='RMSE')
plt.hlines(first_rmse, xmin=min(iter_array), xmax=max(iter_array), colors='r', linestyles='dashed')
plt.xlabel('Number of iterations') 
plt.ylabel('RMSE')
plt.title('RMSE vs number of iterations')
plt.savefig('ex3_rmse.png')
plt.show()
\end{lstlisting}

\item De um modo geral, o early stopping obteve um melhor desempenho em comparação com um número fixo de iterações. Isto é, o RMSE obtido com early stopping é menor do que o RMSE obtido com um número fixo de iterações (20, 50 e 100).
Para o número máximo de iterações igual a 200, o RMSE obtido com early stopping é maior do que o RMSE obtido com um número fixo de iterações. No entanto, a diferença entre os dois RMSE's é muito pequena.

Early stopping é uma técnica utilizada para combater o overfitting. No entanto, pode impedir que o modelo alcance seu potencial total de ajuste se o critério de paragem utilizado não for o mais adequado (por ser demasiado elevado, por exemplo).

Também é importante referir que definir um número fixo de iterações pode ser ineficiente, já que o modelo pode não precisar de tantas iterações para convergir para uma solução aceitável. 

O early stopping a 20\% é um critério de paragem que fornece um certo nível de flexibilidade no treino do modelo. Ele permite que o treino continue até que o modelo comece a demonstrar um claro sinal de overfitting, mas também evita que seja encerrado prematuramente devido a flutuações no erro.


\end{enumerate}

\end{document}
