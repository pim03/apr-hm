{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the ARFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate input from output data and normalize the features using sklearn's minmax scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = df.drop('class', axis=1)\n",
    "target = df['class']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Define purity_score and create 2 arrays, 1 to store purity values and 1 to store silhouette values. In a for loop, create kmeans algorithms with k={2, 3, 4, 5}, train them and calculate purity and silhouette values. At the end, store them in the respective array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, cluster\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency/confusion matrix\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(confusion_matrix, axis=0)) / np.sum(confusion_matrix) \n",
    "\n",
    "silhouettes = []\n",
    "purities = []\n",
    "\n",
    "for k in range(2, 6):\n",
    "    kmeans_algo = cluster.KMeans(n_clusters=k, random_state=0 , n_init= 'auto')\n",
    "    kmeans_model = kmeans_algo.fit(features_scaled)\n",
    "    target_pred = kmeans_model.labels_\n",
    "    purity = purity_score(target, target_pred)\n",
    "    silhouette = metrics.silhouette_score(features_scaled, target_pred)\n",
    "    silhouettes.append(silhouette)\n",
    "    purities.append(purity)\n",
    "    \n",
    "    print(\"Purity score for k = \" , str(k) , \" is \" , purity)\n",
    "    print(\"Silhouette score for k = \" , str(k) , \" is \" , silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the silhouette and purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot Silhouette\n",
    "plt.plot([2,3,4,5], silhouettes, 'o-')\n",
    "plt.title('Silhouette scores for k-means clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.savefig('ex1_silhouette.png')\n",
    "plt.show()\n",
    "\n",
    "#Plot Purity\n",
    "plt.plot([2,3,4,5], purities, 'o-')\n",
    "plt.title('Purity scores for k-means clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Purity score')\n",
    "plt.savefig('ex1_purity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "**i)**\n",
    "\n",
    "Create a PCA object, fit the PCA model to the scaled features and transform the original data into a new dataset (X_pca) with only two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(features_scaled)\n",
    "X_pca = pca.transform(features_scaled)\n",
    "\n",
    "print(\"Components (eigenvectors):\\n\",pca.components_)\n",
    "print(\"Explained variance (eigenvalues) =\",pca.explained_variance_)\n",
    "print(\"Explained variance (ratio) =\",pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii)**\n",
    "\n",
    "Extract the eigenvectors of the first and second principal components. These vectors represent the directions of maximum variance in the new feature space. Calculate the importance of each feature in the new feature space. This importance is determined by taking the Euclidean norm of the coefficients of the feature in the eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "xvector = pca.components_[0] \n",
    "yvector = pca.components_[1]\n",
    "\n",
    "columns = features.columns\n",
    "impt_features = {columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(len(columns))}\n",
    "print(\"Features by importance:\\n\", sorted(zip(impt_features.values(),impt_features.keys()),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "**i)**\n",
    "\n",
    "Visualize side-by-side the data using the ground diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(X_pca[target=='Normal', 0], X_pca[target=='Normal', 1], 'o', markersize=7, alpha=0.6, label='Normal')\n",
    "plt.plot(X_pca[target=='Hernia', 0], X_pca[target=='Hernia', 1], 'o', markersize=7, alpha=0.6, label='Hernia')\n",
    "plt.plot(X_pca[target=='Spondylolisthesis', 0], X_pca[target=='Spondylolisthesis', 1], 'o', markersize=7, alpha=0.6, label='Spondylolisthesis')\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.title('Ground diagnosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii)**\n",
    "\n",
    "Visualize side-by-side the data using the previously learned k = 3 clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_algo = cluster.KMeans(n_clusters=3, random_state=0 , n_init= 'auto')\n",
    "kmeans_model = kmeans_algo.fit(features_scaled)\n",
    "target_pred = kmeans_model.labels_\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=target_pred, alpha=0.6)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.title('k = 3 clustering solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra/Optional Plot**\n",
    "\n",
    "Combine the cluster labels with the original class labels into a DataFrame. Calculate mode class for each cluster and create the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapping = pd.DataFrame({'Cluster': target_pred, 'Class': target})\n",
    "\n",
    "# Calculate the mode class for each cluster\n",
    "cluster_mode = cluster_mapping.groupby('Cluster')['Class'].agg(lambda x: x.mode().iat[0])\n",
    "\n",
    "for cluster in set(target_pred):\n",
    "    data = X_pca[target_pred == cluster]\n",
    "    plt.scatter(data[:, 0], data[:, 1], label=f'Cluster {cluster}', alpha=0.6)\n",
    "\n",
    "plt.title('K-means Clustering')\n",
    "\n",
    "# Create a legend using the calculated mode class for each cluster\n",
    "legend_labels = [f'Cluster {cluster}: {mode_class}' for cluster, mode_class in cluster_mode.items()]\n",
    "plt.legend(legend_labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
